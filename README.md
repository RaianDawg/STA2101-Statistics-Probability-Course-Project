Global Terrorism Statistical Analysis (1970 – 2017)
Project Overview

This project presents a statistical analysis of global terrorism incidents using the Global Terrorism Database (GTD), which records terrorist attacks worldwide from 1970 to 2017. The objective of the study is to explore patterns in how attacks occur, how severe their impacts are, and how different characteristics of incidents relate to human casualties. Rather than focusing on prediction, the project emphasizes statistical reasoning, probability, and data interpretation using real-world data.

The analysis is conducted through multiple milestones, each corresponding to topics covered in the Statistics & Probability course. Together, these milestones provide a structured examination of terrorism data using sampling methods, descriptive statistics, probability theory, and simple linear regression.

Key Features & Methodology

The project is divided into several analytical components, each addressing a specific statistical concept:

Sampling Techniques:
Applied simple random sampling, systematic sampling, stratified sampling, and cluster sampling to understand how different sampling methods affect estimates drawn from large datasets.

Descriptive Statistics:
Constructed frequency distributions, histograms, frequency polygons, and ogives to summarize key numerical variables such as fatalities and injuries.

Measures of Central Tendency and Dispersion:
Analyzed means, medians, variances, and standard deviations to understand the distribution and variability of attack severity.

Probability Analysis:
Defined meaningful real-world events related to attack outcomes and computed empirical probabilities, including intersections, unions, complements, and conditional probabilities.

Bayesian Reasoning and Distribution Analysis:
Used Bayes’ rule and normal distribution concepts to interpret uncertainty and variability in casualties.

Simple Linear Regression:
Performed manual least-squares regression to explore relationships between fatalities and other quantitative variables, focusing on exploratory trends rather than causal claims.

Technologies & Libraries

Python: Primary programming language used for analysis

Pandas: Data loading, cleaning, and manipulation

NumPy: Numerical operations and array-based calculations

Matplotlib & Seaborn: Visualization of distributions, trends, and probabilities

SciPy: Normal distribution functions and probability calculations

Jupyter Notebook / Google Colab: Development and execution environment

Repository Content

Course_Project_Report.pdf: Formal written report summarizing the analysis and findings

Milestone Notebooks (M2 – M7):
Step-by-step Jupyter notebooks covering sampling, descriptive statistics, probability analysis, Bayesian reasoning, and simple linear regression

CSV Outputs: Exported tables generated during probability and sampling analyses

Purpose of the Project

The primary goal of this project is to strengthen practical understanding of statistical and probability concepts using a large, real-world dataset. The emphasis is placed on careful data exploration, correct application of statistical rules, and clear interpretation of results within a realistic context, rather than on complex predictive modeling.
